{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eEcC-hyPQrNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmhJm-JsLhaw",
        "outputId": "39e71d40-6b09-4ef8-8587-0ee3dcf18e15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtKfLfFNZbgE",
        "outputId": "930e5c18-cb98-4754-9abc-cdaa4ea1de2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.9)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPy6nF1v1FZH",
        "outputId": "a9b78ca9-0a5b-47af-cab4-8332c245ac7e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/3.1 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSVbRMf2QeQW",
        "outputId": "9b4c7ea6-ca74-4e0e-9166-6b5113d1710f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Loaded dataset from Google Drive: (243793, 5)\n",
            "\n",
            "Top –5 similar books to first item:\n",
            "\n",
            "Top recommendations for user A100V1W0C8BWOL:\n",
            " - Alfred Hitchcock Presents 12 Stories For Late At Night: 5.00\n",
            " - Lord Of The Flies: 5.00\n",
            " - Their Eyes Were Watching God: 5.00\n",
            " - You Cannot Be Serious: 5.00\n",
            " - To Kill A Mocking Bird: 5.00\n",
            " - The Scarlet Letter: 4.75\n",
            " - The Great Gatsby: 4.00\n",
            " - Huckleberry Finn: 4.00\n",
            " - Inherit The Wind: 4.00\n",
            " - Special Delivery: 4.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but NearestNeighbors was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        " Test Script for Collaborative Book Recommender\n",
        " ---------------------------------------------\n",
        " Loads the pre-sampled & pre-cleaned CSV file from Google Drive\n",
        " and runs the full collaborative filtering + KNN logic.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Constants\n",
        "CSV_PATH = \"SampleData.csv\"\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"vader_lexicon\")\n",
        "\n",
        "# Setup\n",
        "STOP_WORDS = set(stopwords.words(\"english\"))\n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "SIA = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"\\u2714 Loaded dataset:\", df.shape)\n",
        "\n",
        "# Preprocess summaries\n",
        "def preprocess_summary(text):\n",
        "    if pd.isna(text): return \"\"\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [t.lower() for t in tokens if t.isalpha() and t.lower() not in STOP_WORDS]\n",
        "    lemmas = [LEMMATIZER.lemmatize(t) for t in tokens]\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "df[\"cleaned_summary\"] = df[\"review/summary\"].fillna(\"\").apply(preprocess_summary)\n",
        "\n",
        "# TF-IDF + SVD\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "tfidf = vectorizer.fit_transform(df[\"cleaned_summary\"])\n",
        "svd = TruncatedSVD(n_components=100, random_state=42)\n",
        "svd_matrix = svd.fit_transform(tfidf)\n",
        "\n",
        "# Sentiment\n",
        "df[\"sentiment\"] = df[\"review/summary\"].fillna(\"\").apply(lambda x: SIA.polarity_scores(x)[\"compound\"])\n",
        "\n",
        "# Filter active users & popular books\n",
        "MIN_RATINGS = 5\n",
        "user_counts = df[\"User_id\"].value_counts()\n",
        "book_counts = df[\"Title\"].value_counts()\n",
        "df = df[df[\"User_id\"].isin(user_counts[user_counts >= MIN_RATINGS].index)]\n",
        "df = df[df[\"Title\"].isin(book_counts[book_counts >= MIN_RATINGS].index)]\n",
        "\n",
        "# Item KNN\n",
        "item_knn = NearestNeighbors(n_neighbors=6, metric=\"cosine\")\n",
        "item_knn.fit(svd_matrix)\n",
        "\n",
        "# Recommend similar items\n",
        "distances, indices = item_knn.kneighbors(svd_matrix[0].reshape(1, -1))\n",
        "print(\"\\nTop –5 similar books to first item:\")\n",
        "for rank, idx in enumerate(indices[0][1:], 1):\n",
        "    if idx < len(df):\n",
        "        print(f\" {rank}. {df.iloc[idx]['Title']} (dist={distances[0][rank]:.3f})\")\n",
        "\n",
        "# User KNN\n",
        "user_item_matrix = df.pivot_table(index=\"User_id\", columns=\"Title\", values=\"review/score\").fillna(0)\n",
        "user_knn = NearestNeighbors(n_neighbors=6, metric=\"cosine\")\n",
        "user_knn.fit(user_item_matrix)\n",
        "\n",
        "target_uid = user_item_matrix.index[0]\n",
        "dists, user_idxs = user_knn.kneighbors(user_item_matrix.iloc[0].values.reshape(1, -1))\n",
        "neighbors = user_item_matrix.index[user_idxs[0][1:]]\n",
        "neigh_df = df[df[\"User_id\"].isin(neighbors)]\n",
        "already_read = df[df[\"User_id\"] == target_uid][\"Title\"].unique()\n",
        "\n",
        "recs = (\n",
        "    neigh_df[~neigh_df[\"Title\"].isin(already_read)]\n",
        "    .groupby(\"Title\")[\"review/score\"]\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(10)\n",
        ")\n",
        "\n",
        "print(f\"\\nTop recommendations for user {target_uid}:\")\n",
        "for title, score in recs.items():\n",
        "    print(f\" - {title}: {score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3uXvMP1TQfce"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}