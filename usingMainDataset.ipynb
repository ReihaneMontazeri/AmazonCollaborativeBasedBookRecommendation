{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmhJm-JsLhaw",
        "outputId": "9f0e756b-82d5-4800-f864-f37d9e0bb404"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtKfLfFNZbgE",
        "outputId": "930e5c18-cb98-4754-9abc-cdaa4ea1de2d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.9)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTqu9FQZvbId",
        "outputId": "367f2b98-cf83-4bb1-c63b-d9aba1337cd9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzy\n",
            "  Downloading Fuzzy-1.2.2.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fuzzy\n",
            "  Building wheel for fuzzy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzy: filename=Fuzzy-1.2.2-cp311-cp311-linux_x86_64.whl size=220704 sha256=b1a826002649a5147fab870dff3e67e95ae507bf20564b496e3ddbfc0ed008f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/1c/77/28af87176ebf6eb6208c17e64a45a8e48eda4194bd8f605096\n",
            "Successfully built fuzzy\n",
            "Installing collected packages: fuzzy\n",
            "Successfully installed fuzzy-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm78Qnypvlf8",
        "outputId": "dc2fb6ce-855e-4ebc-a6a3-37e27e26593d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPy6nF1v1FZH",
        "outputId": "cbe527ab-7d63-48e7-d92e-8a0121fe4633"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nMtls4qIHJJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        " Collaborative Book Recommendation System\n",
        " ---------------------------------------\n",
        " Cleaned and modular version of the original notebook code.\n",
        "\n",
        " * Downloads the Amazon Books Reviews dataset via KaggleHub.\n",
        " * Performs data reduction, cleaning & preprocessing (title normalization, text cleaning).\n",
        " * Builds a TF–IDF + Truncated SVD content representation and a K‑NN model\n",
        "   for item‑based recommendations.\n",
        " * Builds a user‑item matrix and a K‑NN model for user‑based CF and generates\n",
        "   recommendations with a distance‑weighted score.\n",
        "\n",
        " Usage\n",
        " -----\n",
        " $ python collaborative_recommender.py\n",
        " \"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import kagglehub\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rapidfuzz import fuzz, process\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Globals & constants\n",
        "# ----------------------------------------------------------------------------\n",
        "DATASET = \"mohamedbakhet/amazon-books-reviews\"\n",
        "DATA_DIR = Path(\"data\")\n",
        "SAMPLE_FRACTION = 0.10  # 10 % sample for rapid iteration\n",
        "MIN_RATINGS = 5         # Filter threshold for active users / popular books\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Utility functions\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def download_dataset() -> Path:\n",
        "    path = Path(kagglehub.dataset_download(DATASET))\n",
        "    print(f\"✔ Downloaded dataset to {path}\")\n",
        "    return path\n",
        "\n",
        "\n",
        "def find_csv(root: Path) -> Path:\n",
        "    for fpath in root.rglob(\"*.csv\"):\n",
        "        if fpath.suffix == \".csv\" and fpath.name.lower().endswith(\"rating.csv\"):\n",
        "            print(f\"✔ Found CSV file: {fpath}\")\n",
        "            return fpath\n",
        "    raise FileNotFoundError(\"No rating CSV found in dataset directory.\")\n",
        "\n",
        "\n",
        "def reduce_and_sample_dataset(csv_path: Path, out_path: Path, sample_frac=0.1, random_state=42) -> pd.DataFrame:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.dropna(subset=['User_id'])\n",
        "    df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "    sampled_df = df.sample(frac=sample_frac, random_state=random_state).reset_index(drop=True)\n",
        "    sampled_df.to_csv(out_path, index=False)\n",
        "    print(f\"✔ Saved {sample_frac*100:.0f}% sample to {out_path}\")\n",
        "    return sampled_df\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Text preprocessing helpers\n",
        "# ---------------------------------------------------------------------------\n",
        "STOP_WORDS = set(stopwords.words(\"english\"))\n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "SIA = SentimentIntensityAnalyzer()\n",
        "\n",
        "def preprocess_summary(text: str) -> str:\n",
        "    text = text or \"\"\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [w.lower() for w in tokens if w.isalpha() and w.lower() not in STOP_WORDS]\n",
        "    lemmas = [LEMMATIZER.lemmatize(t) for t in tokens]\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "\n",
        "def clean_title(text) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\s*\\(.*?\\)\\s*\", \"\", text)\n",
        "    text = text.replace(\"&\", \"and\")\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def lotr_title(text: str) -> str:\n",
        "    return \"the lord of the rings\" if isinstance(text, str) and re.match(r\"the lord of the rings.*\", text) else text\n",
        "\n",
        "\n",
        "def _best_match(title: str, choices: np.ndarray, threshold: int = 80) -> str:\n",
        "    \"\"\"Return the best fuzzy match exceeding *threshold* or the original title.\"\"\"\n",
        "    result = process.extractOne(title, choices, scorer=fuzz.token_sort_ratio)\n",
        "    if result is None:\n",
        "        return title  # no candidates\n",
        "    match = result[0]\n",
        "    score = result[1]\n",
        "    return match if score >= threshold else title\n",
        "\n",
        "\n",
        "def fuzzy_dedupe_titles(chunk: pd.Series) -> pd.Series:\n",
        "    unique_titles = chunk.unique()\n",
        "    return chunk.apply(lambda x: _best_match(x, unique_titles))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Main ETL pipeline\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def basic_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.drop(columns=[\n",
        "        \"Price\", \"profileName\", \"review/helpfulness\", \"review/time\", \"review/text\",\n",
        "    ], errors=\"ignore\")\n",
        "    df[\"Title\"] = df[\"Title\"].apply(clean_title).apply(lotr_title)\n",
        "    df = df[df[\"Title\"].astype(bool)]\n",
        "    chunks = np.array_split(df, 10)\n",
        "    processed = []\n",
        "    for c in chunks:\n",
        "        c[\"Title\"] = fuzzy_dedupe_titles(c[\"Title\"])\n",
        "        processed.append(c)\n",
        "    df = pd.concat(processed)\n",
        "    df[\"Title\"] = df[\"Title\"].str.title()\n",
        "    return df.drop_duplicates()\n",
        "\n",
        "def enrich_text_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray]:\n",
        "    df[\"cleaned_summary\"] = df[\"review/summary\"].fillna(\"\").apply(preprocess_summary)\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    tfidf = vectorizer.fit_transform(df[\"cleaned_summary\"])\n",
        "    svd = TruncatedSVD(n_components=100, random_state=RANDOM_STATE)\n",
        "    svd_matrix = svd.fit_transform(tfidf)\n",
        "    df[\"sentiment\"] = df[\"review/summary\"].fillna(\"\").apply(lambda x: SIA.polarity_scores(x)[\"compound\"])\n",
        "    return df, svd_matrix\n",
        "\n",
        "\n",
        "def filter_active(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    active_users = df[\"User_id\"].value_counts()\n",
        "    active_users = active_users[active_users >= MIN_RATINGS].index\n",
        "    popular_books = df[\"Title\"].value_counts()\n",
        "    popular_books = popular_books[popular_books >= MIN_RATINGS].index\n",
        "    return df[df[\"User_id\"].isin(active_users) & df[\"Title\"].isin(popular_books)]\n",
        "\n",
        "\n",
        "def build_item_knn(svd_matrix: np.ndarray) -> NearestNeighbors:\n",
        "    knn = NearestNeighbors(n_neighbors=6, metric=\"cosine\")\n",
        "    knn.fit(svd_matrix)\n",
        "    return knn\n",
        "\n",
        "\n",
        "def build_user_knn(user_item: pd.DataFrame) -> NearestNeighbors:\n",
        "    model = NearestNeighbors(n_neighbors=6, metric=\"cosine\")\n",
        "    model.fit(user_item)\n",
        "    return model\n",
        "\n",
        "\n",
        "def user_item_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    ui = df.pivot_table(index=\"User_id\", columns=\"Title\", values=\"review/score\")\n",
        "    return ui.fillna(0)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Demo / script execution\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def main():\n",
        "    for pkg in (\"punkt\", \"wordnet\", \"stopwords\", \"vader_lexicon\"):\n",
        "        nltk.download(pkg, quiet=True)\n",
        "\n",
        "    DATA_DIR.mkdir(exist_ok=True)\n",
        "    csv_path = find_csv(download_dataset())\n",
        "\n",
        "    subset_path = DATA_DIR / \"amazonCollaborativeSubset.csv\"\n",
        "    sampled = reduce_and_sample_dataset(csv_path, subset_path, SAMPLE_FRACTION, RANDOM_STATE)\n",
        "\n",
        "    cleaned = basic_clean(sampled)\n",
        "    enriched, svd_matrix = enrich_text_features(cleaned)\n",
        "    filtered = filter_active(enriched)\n",
        "\n",
        "    out_csv = DATA_DIR / \"ratings2_processed.csv\"\n",
        "    filtered.to_csv(out_csv, index=False)\n",
        "    print(f\"✔ Saved cleaned sample to {out_csv}\")\n",
        "\n",
        "    item_knn = build_item_knn(svd_matrix)\n",
        "    distances, idxs = item_knn.kneighbors(svd_matrix[0].reshape(1, -1))\n",
        "    print(\"\\nTop‑5 similar books to first item:\")\n",
        "    for rank, idx in enumerate(idxs[0][1:], 1):\n",
        "        print(f\" {rank}. {cleaned.iloc[idx]['Title']} (dist={distances[0][rank]:.3f})\")\n",
        "\n",
        "    ui = user_item_matrix(filtered)\n",
        "    user_knn = build_user_knn(ui)\n",
        "    dists, user_idxs = user_knn.kneighbors(ui.iloc[0].values.reshape(1, -1))\n",
        "    target_uid = ui.index[0]\n",
        "    neighbors = ui.index[user_idxs[0][1:]]\n",
        "    neighbor_df = filtered[filtered[\"User_id\"].isin(neighbors)]\n",
        "    already_read = filtered[filtered[\"User_id\"] == target_uid][\"Title\"].unique()\n",
        "    recs = (neighbor_df[~neighbor_df[\"Title\"].isin(already_read)]\n",
        "            .groupby(\"Title\")[\"review/score\"].mean()\n",
        "            .sort_values(ascending=False)[:10])\n",
        "\n",
        "    print(f\"\\nTop recommendations for user {target_uid}:\")\n",
        "    for title, score in recs.items():\n",
        "        print(f\" - {title}: {score:.2f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo0x2y7XLYZ2",
        "outputId": "1d3db5a7-73cf-49a8-b86f-f18569005009"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Downloaded dataset to /kaggle/input/amazon-books-reviews\n",
            "✔ Found CSV file: /kaggle/input/amazon-books-reviews/Books_rating.csv\n",
            "✔ Saved 10% sample to data/amazonCollaborativeSubset.csv\n",
            "✔ Saved cleaned sample to data/ratings2_processed.csv\n",
            "\n",
            "Top‑5 similar books to first item:\n",
            " 1. Death In Hyde Park (dist=0.000)\n",
            " 2. Poison Mind (dist=0.000)\n",
            " 3. Imitation In Death (dist=0.000)\n",
            " 4. Death Match (dist=0.000)\n",
            " 5. Seduction In Death (dist=0.000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but NearestNeighbors was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top recommendations for user A100V1W0C8BWOL:\n",
            " - Alfred Hitchcock Presents 12 Stories For Late At Night: 5.00\n",
            " - Pride And Prejudice: 5.00\n",
            " - Lord Of The Flies: 5.00\n",
            " - Their Eyes Were Watching God: 5.00\n",
            " - You Cannot Be Serious: 5.00\n",
            " - To Kill A Mocking Bird: 5.00\n",
            " - The Great Gatsby: 4.00\n",
            " - Huckleberry Finn: 4.00\n",
            " - Inherit The Wind: 4.00\n",
            " - Spark Notes Our Town: 4.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P6z1yKiGVAUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}